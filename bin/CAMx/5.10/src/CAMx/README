--------------------------------------------------------------------------------
DO YOU USE A 64-BIT COMPILER?

CAMx can be run on a 64-bit OS using a 32-bit compiler,
however use of a 64-bit compiler will result in speed benefits. 
The default CAMx makefile utilizes compiler options for the
PFG and Intel 32-bit compilers. We offer an alternative makefile
that includes specific 64-bit compiler flags for PGF.

There have been a number of changes with the latest Portland
Group Compiler (64 bit version) that may cause a compilation
of CAMx to fail on your system using the default makefile.

If you encounter any of the following errors while compiling
CAMx and you are using a PGF 64 bit compiler, try using the 
alternative Makefile for the 64-bit PGF compiler: (Makefile.pg_64bit)

relocation truncated to fit: R_X86_64_PC32 against symbol

/usr/bin/ld: cannot find -lnumao

/usr/lib64/libpthread.a(pthread_cond_timedwait.o): In function `pthread_cond_timedwait':
(.text+0xa7): undefined reference to `__vdso_clock_gettime'

--------------------------------------------------------------------------------
UPGRADING FROM CAMx v4 TO v5

V5.x is a major update from previous releases (v4.x).
Version 5 introduces parallel processing on distributed
memory (networked) computer cluster environments using the
Message Passing Interface (MPI) protocol. CAMx continues to
offer parallel processing on shared-memory (multi-core)
computers using the OpenMP (OMP) protocol.  Both MPI and OMP
can be used in combination.

To use MPI you must have the MPICH or MPICH2 utility installed
on your system.

There are no changes to input file formats from versions
5.0x, 4.5x, v4.4x, or v4.3x.  There are no changes to core model
output file formats from versions 5.0x, 4.5x or v4.4x; new
OSAT/PSAT output deposition files can now be generated.

Visit www.camx.com to obtain the latest code, version-specific input
files (e.g., chemistry parameters) and all test case I/O.

--------------------------------------------------------------------------------

INTRODUCTION

This is the README file for the CAMx model.  Topics covered are the 
license agreement, compiling CAMx, the test case, and multi-processor
support.  

Further details are in the CAMx User's Guide, available from 
http://www.camx.com, and the release notes.

Send questions or comments via email to: ask-camx@environ.org
Please read the documentation first.


LICENSE AGREEMENT

Read the file LICENSE included with the source code and read the web 
page where the CAMx source code is posted for download 
(http://www.camx.com). You agree to the CAMx license terms by 
downloading the source code and/or using the CAMx model.


USING THE CAMx MAKEFILE

NOTE: If you will be using the Message Passing Interface (MPI) you
will need to make sure the path to your mpich distribution is set
correctly in the standard CAMx Makefile, as well as the Makefile in
the MPI/util directory. In both cases, just set the MPI_INST variable
to the path of the installation on your system. The default is:
/usr/local/mpich

The syntax for building CAMx is:

make platform <DOMAIN=domain_name> <MPI=true> <HDF=true>

The CAMx makefile has four arguments, one mandatory ("platform") and
three optional (DOMAIN, MPI, and HDF).

"platform" specifies what type of workstation you have and must
be one of the following:

dec         -- DEC alpha workstation with digital Unix
               (uses big_endian representation for FORTRAN unformatted 
               files rather than the DEC default, which is little
               endian)
sun         -- Sun Ultra Sparcstation
sgi         -- SGI workstation
sgiomp      -- SGI workstation with compiler flags to enable OMP
ibm         -- IBM workstation
hp          -- HP/UX workstation
pg_linux    -- ix86 PC running Linux and using FORTRAN compiler from the 
               The Portland Group (pgf77)
               (uses big_endian representation for FORTRAN unformatted 
               files rather than the PC default, which is little
               endian)
pg_linuxomp -- Linux workstation with pgf77 compiler flags to enable OMP
i_linux     -- ix86 PC running Linux and using FORTRAN compiler from 
               Intel (ifort)
               (uses big_endian representation for FORTRAN unformatted 
               files rather than the PC default, which is little
               endian)
i_linuxomp  -- Linux workstation with Intel compiler flags to enable OMP
ab_linux    -- Linux Mac OSX 64-bit operating system using the Absoft compiler

"domain_name" is a string identifying which camx.prm include file to 
use in the compilation.  The camx.prm file, found in the ./Includes/ 
subdirectory, contains the domain definition parameters for your
simulation (See the CAMx User's Guide for more details).  This
makefile syntax is designed to allow you to keep several different
CAMx configurations (camx.prm files) and selectively build executables 
for each application.  The "DOMAIN=domain_name" argument is optional, 
and if omitted the default "camx.prm.v5.10" configuration will be used.
This feature is retained for legacy purposes. Since it is no longer 
necessary to configure the grid parameters specifically for your modeling 
domain, the default camx.prm file distributed with the source code should 
be sufficient for your application. However, you may want to customize 
some applications, for example with probing tools or without probing tools, 
and it is convenient to be able to distinguish between these executables.

"HDF=true" instructs the make utility to include HDF5 libraries in the CAMx
executable.  This requires that HDF5 libraries have been installed and built
on the machine that is running this Makefile script and compiling CAMx.
You should check that the variables HDF_LIB and HDF_INC in the Makefile are
set correctly.  This only enables CAMx to generate HDF5 output files; the user 
maintains control of whether to output HDF5 concentration files by setting
the "HDF_Format_Output" variable in the CAMx CONTROL namelist when
the model is run.  If "HDF=true" is not specified, the make utility will 
proceed to compile CAMx without HDF5 libraries.

"MPI=true" will compile CAMx to utilize MPI parallel processing.  This
requires that the MPICH package has been installed and built on the machine
that is running this Makefile script and compiling CAMx.  You should check
that the variable MPI_INST in the Makefile is set correctly to the MPICH
installation path.  If the MPI flag is not set on the command line, the
Makefile script will ignore the MPICH library and CAMx will not be able to
run with MPI parallelization.  Currently, the Makefile only supports building
an MPI version of CAMx using the Portland Group and Intel Fortran90 compilers.

For example, to build CAMx on a Linux workstation using the Portland Group
compiler with OMP invoked, using the include file ./Includes/camx.prm.myapp,
and to use MPI paralellization and HDF5 output, enter the make command:

make pg_linuxomp DOMAIN=myapp MPI=true HDF5=true

The executable will be named:

CAMx.myapp.MPI.pg_linuxomp

If you need to rebuild CAMx using different makefile arguments
we recommend typing "make clean" between builds.  Make clean will
delete all existing object files forcing a complete re-build.

If your workstation is not supported by the standard makefile, you will
need to modify the Makefile for use with your system.  If so, please 
mail the working Makefile along with a description of your computer 
system to ask-camx@environ.org so that we can share this information.


CAMx TEST CASE

A test case is provided to help new CAMx User's get started and
permit performance benchmarking.  The test case is for the Midwest
U.S. on June 3-4 2002.  The test case has 1 nested fine grid and 
requires about 650 MBytes of memory.

The test case (2 days of simulation) including inputs and
outputs is available via FTP at www.camx.com or at the CAMx FTP site:

        Hostname: ftp.environ.org
        Username: anonymous
        Directory: CAMx_testcase 

See the README file on the FTP server to determine which files you 
need to download.  You can compare your results to the test case 
benchmark by displaying the concentration outputs (using PAVE, for 
example).  You can perform a more rigorous check by comparing the 
coarse grid average concentration outputs using the "AVGDIF" program.  
This program is on the CAMx web site in the download/support programs
area.  We expect concentrations to agree within the limits of single
precision calculation, namely relative errors of about 1 in 100,000.


MULTI-PROCESSOR SUPPORT

CAMx supports the distribution of program control to multiple processors,
an approach generally referred to as "parallel processing", as a way to
maximize the speed of the simulation.  CAMx uses two protocols to this,
and either or both may be used depending on your system configuration:

a) OpenMP (OMP) compiler directives for shared-memory platforms (e.g,
multi-core computers sharing common memory);

b) Message Passing Interface (MPI) using MPICH or MPICH2 utilities for
distributed-memory systems (e.g., networked or cluster environments).


OMP PARALLEL PROCESSING

This only works for shared memory systems.  For example, you can take
advantage of a dual, quad, or dual-quad core processors on Linux PCs,
but not a Linux cluster.  The OMP option has been tested for Linux and
SGI workstations, and the makefile has options "sgiomp", "pg_linuxomp",
and "i_linuxomp".  You must build a CAMx executable with OMP enabled
before you can use multiple processors.

When running the mulitprocessor version (OMP) on a Linux platform,
you must set some environment variables in the job script to take
advantage of multiprocessor capability:
 
#/bin/csh
setenv OMP_NUM_THREADS 4

There have been problems with some parallel programs on Linux systems
when the per-thread stack size is set to the default (2MB). If you have
unexplained failures (segmentation fault/violation), try setting this
environment variable to a larger value, for example:

setenv MPSTKZ 32M
 
For large applications, you will probably also want to increase the
stack size available to the shell executing the model by using the
limit command:
 
limit stacksize unlimited
 

MPI PARALLEL PROCESSING

The MPI version of CAMx was designed using a "master/slave" parallel
processing approach.  The CPU on which the program is launched (process ID 0
under MPI) serves as the master node and will not make any actual model
integration computations for any part of the modeling domain.  This process
will perform all of the model setup, the vast majority of I/O, and manage the
communication between the compute nodes.  Since the master node handles the
important I/O it is the only CPU which needs access to the disk volume
containing the input files and the location of the output directory.  This
approach allows for minimal amount of network traffic to the nodes on the
cluster by eliminating the need for the compute nodes to manage NFS mounts.
The master node may need access to the LAN for data access, but the compute
nodes only need access to the internal cluster network.  However, the compute
nodes will need access to a copy of the executable program.  This can be
accomplished in a number of ways: (1) have an NFS mount on the master node
accessible to the internal cluster network and launch the model from that
location; or (2) port a copy of the executable program, using rcp or scp,
to the user's home directory on each compute node and launch the model from
the user's home directory on the master node.

The MPI version of CAMx uses the same job script as the standard version, with
a few modifications to support multi-processing.  Here, we explain the
approach we have taken.  We provide this as guidance; it is not necessary to
follow these steps to run CAMx/MPI.  Others may have a different way of
handling the MPI portion of the execution.

1. Put the CAMx input files somewhere on the network that is accessible
   by the computer that will serve as the master node.  Also, identify a
   location where the output will be written.
2. Create a directory on the local disk of the master node that will serve
   as the launching point for the CAMx execution.  Copy the standard CAMx
   job script, and any scripting support programs (such as j2g) to this
   location.
3. Modify the job script so that the variable identifying the executable
   program points to the local directory.
4. Modify the job script so that the pathnames for input and output files
   are correct.
5. Add a section, similar to the one shown below, to the top of the job
   script:

For MPICH1 use the following:

cat << ieof > nodes
10.1.4.2
10.1.4.3
10.1.4.4
10.1.4.5
10.1.4.6
10.1.4.2
10.1.4.3
10.1.4.4
ieof
set NUMPROCS = `wc -l nodes | awk '{print $1+1}'`

This will create a file containing the IP addresses of the computers that
will get a computational slice of the domain on which to work.  NOTE: THE
MASTER NODE SHOULD NOT BE LISTED and will not get a computational slice.
The computer on which the program is launched is included in the MPI
simulation by default and is given a process ID of 0.  Notice that some
computers are listed twice.  This example is utilizing a cluster containing
dual processor computers.  The computer must be listed once for each CPU
that is to be used in the simulation.  Notice that the NUMPROCS variable is
assigned to a value equal to one more than the number of computers listed.
This is because the master node is included in the count of processors.
This example will utilize 9 processors: one to serve as the master node
and 8 that will receive a computational slice.  If the names of the computers
are known to the master node, hostnames can be used in place of IP addresses.

For MPICH2 use the following:

cat << ieof > nodes
10.1.4.2:3
10.1.4.3:2
10.1.4.4:2
10.1.4.5:2
ieof
set NUMPROCS = 9
set RING = `wc -l nodes | awk '{print $1}'

Notice that the syntax for each entry differs from the MPICH1 version
in that the addresses are followed by a colon and a numeric value.
This is the convention adopted in MPICH2 for utilizing several processors
on a single host.  Do not list a machine multiple times in the nodes
file.  Unlike MPICH1 the NUMPROCS variable is set to the number the
total nodes listed after the colon.  In this example, the nodes file
will also be used to launch a node "ring" using the MPICH2 utilities.
For this reason you must include the master node in the list - the
master node must be part of the MPICH ring.


6. Change the line that launches the executable program, at the bottom
   of the script.

For MPICH1 use the following:

mpirun -v -machinefile nodes -np $NUMPROCS $EXEC

This will use the mpirun utility to launch the CAMx model using the file
called "nodes" to identify the computers to be included.  Of course, this
assumes that the mpirun utility is in the current user's path.  In fact,
the mpich package must be installed on each compute node and the mpirun
utility must be available in the user's path on each system.  A complete
path to mpirun, such as /usr/local/bin/mpirun, could be used instead.

For MPICH2 use the following:

mpdboot -n $RING -f nodes --verbose
mpiexec -machinefile nodes -np $NUMPROCS $EXEC
mpdallexit

The mpdboot command will initialize the MPD environment, establishing 
a ring using all of the machines listed in the nodes file. The mpiexec 
command will launch the model using the specified number of processors, 
utilizing the machines in the established ring. The mpdallexit command 
will terminate the ring. If your script is looping over several days, 
which is typical, you should put the mpdboot command before the start of 
the loop, the mpiexec command inside the loop, and the mpdallexit command 
after the point where the loop is terminated.


7. Implementing Open-MP (OMP) with an MPI application.

To use a hybrid MPI/OMP approach, follow all of the steps for running an
MPI application.  In addition, add the environment variable for the number
of OMP threads to the CAMx job script.

setenv OMP_NUM_THREADS 2

This will utilize both MPI and OMP in the simulation.  The domain will be
divided into slices in the usual way.  But when operating on a slice, the
host will spawn multiple threads to parallelize the portions of the code
where OMP parallelization has been included.
