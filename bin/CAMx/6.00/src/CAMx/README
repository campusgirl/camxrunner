INTRODUCTION
------------

This is the README file for the CAMx model that includes information about:
1) License agreement
2) CAMx e-mail contacts
3) Upgrading to the latest version of CAMx
4) The distrubuted test case
4) Compiling CAMx
6) Configuring CAMx for multi-processor parallelization

Further details about CAMx are provided in the Release Notes and in the CAMx
User's Guide, available from:

http://www.camx.com.


LICENSE AGREEMENT
-----------------

Read the file LICENSE included with the source code and read the web page where
the CAMx source code is posted for download (http://www.camx.com). You agree to
the CAMx license terms by downloading the source code and/or using the CAMx 
model.


CAMx E-MAIL CONTACTS
--------------------

Join the CAMx User's Group by sending an e-mail message to:

 majordomo "at" environ "dot" org

Put the words "subscribe CAMxusers" in the body of the message.  When you
subscribe, Majordomo will send you a reply that provides details on how to use
this list server.

The "camxusers" group is meant for broadcasting useful information about CAMx
and its pre- and post-processors to all users that subscribe to this group, or
to ask about available datasets or issues that other users may have come across.
It is not meant for asking how to run CAMx, how to prepare inputs, or why CAMx
arrived at a particular prediction.

              ** PLEASE REFER TO THE DOCUMENTATION FIRST **

Please direct specific comments or questions about problems or bugs with CAMx or
its support software to:

 ask-camx "at" environ "dot" org

which comes directly to the developers.


UPGRADING TO CAMx V6 FROM PREVIOS VERSIONS
------------------------------------------

V6.x is a major update from previous releases (v5.x).  Version 6 introduces
a consistent I/O file format for all large gridded arrays, which is similar to 
the CAMx/UAM binary Fortran file structure used previously for emissions, output
concentrations, etc.  The file header records now hold more information about 
grid configuration, time zone, and some variable-specific flags.  Otherwise the
file structures are unchanged.  Note that the meteorological input fields are 
now in the updated CAMx/UAM format. Met variables are grouped as follows:

2-D time-invariant: 26-cat landuse, topography, LAI (optional)
2-D time-variant: surface temperature, snow cover
3-D time-variant: height grid, pressure, temperature, humidity, U/V winds
3-D diffusivity
3-D cloud/rain

Pre-existing software that reads CAMx/UAM format will continue to be
able to read these files, including the new meteorologial files, but will ignore
the new header variables unless otherwise modified to explicitly read them.  
The meteorological pre-processors (WRFCAMx, MM5CAMx, RAMSCAMx) have been 
revised to generate the new met files.  See the User's Guide for more 
information on file formats.

**      CAMx WILL BE ABLE TO READ OLDER EMISSIONS AND IC/BC FILES.       **
** CAMx WILL ONLY READ THE NEW MET FORMATS, SO YOU MUST REGENERATE THOSE **
**    FILES USING THE NEW PRE-PROCESSORS, OR CONVERT OLD FILES USING     **
**                     THE NEW "METCONVERT" PROGRAM.                     **

V6 also introduces a revised manner in which photolysis rates are generated
and used in CAMx.  The albedo/haze/ozone column file now only contains
ozone column and optionally land/ocean mask.  Albedo and haze patterns are 
determined by CAMx automatically.  The AHOMAP pre-processor has been modified
accordingly and is now called O3MAP.  The TUV photolysis pre-processor 
has been revised to read the ozone column file and generates a lookup table as a
function of solar zenith, altitude, ozone column, albedo, and terrain height 
(new).  Photolysis rates are generated for a default haze optical depth profile.
CAMx interpolateis lookup table values to cell-specific values of these 
variables.  All cloud and haze adjustments are made using the in-line TUV.

**               CAMx WILL NOT READ OLD AHO AND TUV FILES                **
**            YOU MUST RUN O3MAP AND TUV TO GENERATE NEW FILES           **

The CAMx namelist control file has changed significantly in this release; refer
to the namelist template provided with the source code and the User's Guide for
a full list of namelist variables.  Some namelist variables have been removed,
some new variables are added.

Visit www.camx.com to obtain the latest code, version-specific input files 
(e.g., chemistry parameters) and all test case I/O.


CAMx TEST CASE
--------------

A test case is provided to help new CAMx User's get started and permit
performance benchmarking.  The test case is for the Midwest U.S. on June 3-4
2002.  The test case has 1 nested fine grid and requires about 650 MBytes of
memory.  The test case includes inputs and outputs for 2 days of simulation,
and is available at www.camx.com.

See the dowloads page on the web site to determine which files you need to
download.  You can compare your results to the test case benchmark by displaying
the concentration outputs using graphics programs.  You can perform a more
rigorous check by comparing the concentration outputs using the "AVGDIF" program.
This program is on the CAMx web site in the download/ support programs area.
We expect concentrations to agree within the limits of single precision
calculation, namely relative errors of about 1 in 100,000.


USING THE CAMx MAKEFILE
-----------------------

The default CAMx makefile utilizes compiler options for the Portland Group (PGF)
and Intel 64-bit compilers.  CAMx can still be run on platforms using a 32-bit 
compiler; we provide an alternative makefile that includes specific 32-bit
compiler flags (Makefile.32bit).

The syntax for building CAMx is:

make platform <DOMAIN=myapp> <MPI=true> <HDF=true>

The CAMx makefile has four arguments, one mandatory ("platform") and three 
optional (DOMAIN, MPI, and HDF).

"platform" specifies what type of compiler you have and must be one of the
following:

pg_linux    -- ix86 PC running Linux and using FORTRAN compiler from the 
               The Portland Group (pgf77)
               (uses big_endian representation for FORTRAN unformatted 
               files rather than the PC default, which is little
               endian)
pg_linuxomp -- Linux workstation with pgf77 compiler flags to enable OMP
i_linux     -- ix86 PC running Linux and using FORTRAN compiler from 
               Intel (ifort)
               (uses big_endian representation for FORTRAN unformatted 
               files rather than the PC default, which is little
               endian)
i_linuxomp  -- Linux workstation with Intel compiler flags to enable OMP
ab_linux    -- Mac OSX 64-bit operating system using the Absoft compiler
ab_linuxomp -- Mac OSX 64-bit using the Absoft compiler with OMP

"domain_name" is a string identifying which camx.prm include file to use in the 
compilation.  The camx.prm file, found in the ./Includes/ subdirectory, contains
certain compile-time flags and array dimensions (See the CAMx User's Guide for 
more details).  This makefile syntax is designed to allow you to keep several 
different CAMx configurations (camx.prm files) and selectively build executables
for each application.  The "DOMAIN=myapp" argument is optional, and if 
omitted the default "camx.prm.v6.00" configuration will be used. The default 
camx.prm file distributed with the source code should be sufficient for your 
application. However, you may want to customize some applications, for example 
with probing tools, and it is convenient to be able to distinguish between these
executables.

"HDF=true" instructs the make utility to include HDF5 libraries in the CAMx
executable.  This requires that HDF5 libraries have been installed and built on
the machine that is running this Makefile script and compiling CAMx.  You should
check that the variables HDF_LIB and HDF_INC in the Makefile are set correctly. 
This only enables CAMx to generate HDF5 output files; the user maintains 
control of whether to output HDF5 concentration files by setting the 
"HDF_Format_Output" variable in the CAMx CONTROL namelist when the model is run.
If "HDF=true" is not specified, the make utility will proceed to compile CAMx 
without HDF5 libraries.

"MPI=true" will compile CAMx to utilize MPI parallel processing.  This requires
that the MPICH package has been installed and built on the machine that is 
running this Makefile script and compiling CAMx.  You should check that the 
variable MPI_INST in the Makefile is set correctly to the MPICH installation 
path.  If the MPI flag is not set on the command line, the Makefile script 
will ignore the MPICH library and CAMx will not be able to run with MPI 
parallelization.  Currently, the Makefile only supports building an MPI version 
of CAMx using the Portland Group and Intel Fortran90 compilers.

NOTE: If you will be using the Message Passing Interface (MPI) you will need to
      make sure the path to your mpich distribution is set correctly in the 
      standard CAMx Makefile, as well as the Makefile in the MPI/util directory.
      In both cases, just set the MPI_INST variable to the path of the 
      installation on your system. The default is: /usr/local/mpich.

For example, to build CAMx on a Linux workstation using the Portland Group
compiler with OMP invoked, using the include file ./Includes/camx.prm.myapp,
and to use MPI paralellization and HDF5 output, enter the make command:

make pg_linuxomp DOMAIN=myapp MPI=true HDF5=true

The executable will be named:

CAMx.myapp.MPI.pg_linuxomp

If you need to rebuild CAMx using different makefile arguments we recommend 
typing "make clean" between builds.  Make clean will delete all existing object 
files forcing a complete re-build.

If your workstation is not supported by the standard makefile, you will need to 
modify the Makefile for use with your system.  If so, please mail the working 
Makefile along with a description of your computer system to:

 ask-camx "at" environ "dot" org

so that we can share this information.

Large applications of CAMx (large/many grids, big Probing Tool configurations)
can result in a compile-time "relocation" error.  If this occurs, add the
following flag to the PGF90 or IFORT flags list in the makefile:

-mcmodel=medium


MULTI-PROCESSOR SUPPORT
-----------------------

CAMx supports the distribution of program control to multiple processors, an 
approach generally referred to as "parallel processing", as a way to maximize 
the speed of the simulation.  CAMx uses two protocols to this, and either or 
both may be used depending on your system configuration:

a) OpenMP (OMP) compiler directives for shared-memory platforms (e.g, multi-core
   computers sharing common memory);

b) Message Passing Interface (MPI) using MPICH or MPICH2 utilities for 
   distributed-memory systems (e.g., networked or cluster environments).


OMP PARALLEL PROCESSING

This only works for shared memory systems.  For example, you can take advantage 
of multi-core processors, but not a networked cluster.  The OMP option has been
tested for Linux and SGI workstations, and the makefile has options "sgiomp", 
"pg_linuxomp", and "i_linuxomp".  You must build a CAMx executable with OMP 
enabled before you can use multiple processors.

When running the mulitprocessor version (OMP) on a Linux platform, you must set 
some environment variables in the job script to take advantage of multiprocessor
capability:
 
#/bin/csh
setenv OMP_NUM_THREADS 4

There have been problems with some parallel programs on Linux systems when the 
per-thread stack size is set to the default (2MB). If you have unexplained 
failures (segmentation fault/violation), try setting this environment variable 
to a larger value, for example:

setenv MPSTKZ 32M
 
For large applications, you will probably also want to increase the stack size 
available to the shell executing the model by using the limit command:
 
limit stacksize unlimited
 

MPI PARALLEL PROCESSING

The MPI version of CAMx was designed using a "master/slave" parallel processing 
approach.  The CPU on which the program is launched (process ID 0 under MPI) 
serves as the master node and will not make any actual model integration 
computations for any part of the modeling domain.  This process will perform 
all of the model setup, the vast majority of I/O, and manage the communication 
between the compute nodes.  Since the master node handles the important I/O it 
is the only CPU which needs access to the disk volume containing the input files
and the location of the output directory.  This approach allows for minimal 
amount of network traffic to the nodes on the cluster by eliminating the need 
for the compute nodes to manage NFS mounts.  The master node may need access 
to the LAN for data access, but the compute nodes only need access to the 
internal cluster network.  However, the compute nodes will need access to a copy
of the executable program.  This can be accomplished in a number of ways: (1) 
have an NFS mount on the master node accessible to the internal cluster network
and launch the model from that location; or (2) port a copy of the executable 
program, using rcp or scp, to the user's home directory on each compute node and
launch the model from the user's home directory on the master node.

The MPI version of CAMx uses the same job script as the standard version, with
a few modifications to support multi-processing.  Here, we explain the approach 
we have taken.  We provide this as guidance; it is not necessary to follow these
steps to run CAMx/MPI.  Others may have a different way of handling the MPI 
portion of the execution.

1. Put the CAMx input files somewhere on the network that is accessible by the 
   computer that will serve as the master node.  Also, identify a location where
   the output will be written.
2. Create a directory on the local disk of the master node that will serve as 
   the launching point for the CAMx execution.  Copy the standard CAMx job 
   script, and any scripting support programs (such as j2g) to this location.
3. Modify the job script so that the variable identifying the executable program
    points to the local directory.
4. Modify the job script so that the pathnames for input and output files are 
   correct.
5. Add a section, similar to the one shown below, to the top of the job script:

For MPICH1 use the following:

cat << ieof > nodes
10.1.4.2
10.1.4.3
10.1.4.4
10.1.4.5
10.1.4.6
10.1.4.2
10.1.4.3
10.1.4.4
ieof
set NUMPROCS = `wc -l nodes | awk '{print $1+1}'`

This will create a file containing the IP addresses of the computers that will 
get a computational slice of the domain on which to work.  NOTE: THE MASTER NODE
SHOULD NOT BE LISTED and will not get a computational slice.  The computer on 
which the program is launched is included in the MPI simulation by default and 
is given a process ID of 0.  Notice that some computers are listed twice.  This 
example is utilizing a cluster containing dual processor computers.  The 
computer must be listed once for each CPU that is to be used in the simulation. 
Notice that the NUMPROCS variable is assigned to a value equal to one more than 
the number of computers listed.  This is because the master node is included 
in the count of processors.  This example will utilize 9 processors: one to 
serve as the master node and 8 that will receive a computational slice.  If the 
names of the computers are known to the master node, hostnames can be used in 
place of IP addresses.

For MPICH2 use the following:

cat << ieof > nodes
10.1.4.2:3
10.1.4.3:2
10.1.4.4:2
10.1.4.5:2
ieof
set NUMPROCS = 9
set RING = `wc -l nodes | awk '{print $1}'

Notice that the syntax for each entry differs from the MPICH1 version in that 
the addresses are followed by a colon and a numeric value.  This is the 
convention adopted in MPICH2 for utilizing several processors on a single host. 
Do not list a machine multiple times in the nodes file.  Unlike MPICH1 the 
NUMPROCS variable is set to the number the total nodes listed after the colon.
In this example, the nodes file will also be used to launch a node "ring" using 
the MPICH2 utilities.  For this reason you must include the master node in the 
list - the master node must be part of the MPICH ring.

6. Change the line that launches the executable program, at the bottom of the 
   script.

For MPICH1 use the following:

mpirun -v -machinefile nodes -np $NUMPROCS $EXEC

This will use the mpirun utility to launch the CAMx model using the file called 
"nodes" to identify the computers to be included.  Of course, this assumes that 
the mpirun utility is in the current user's path.  In fact, the mpich package 
must be installed on each compute node and the mpirun utility must be available 
in the user's path on each system.  A complete path to mpirun, such as 
/usr/local/bin/mpirun, could be used instead.

For MPICH2 use the following:

mpdboot -n $RING -f nodes --verbose
mpiexec -machinefile nodes -np $NUMPROCS $EXEC
mpdallexit

The mpdboot command will initialize the MPD environment, establishing a ring 
using all of the machines listed in the nodes file. The mpiexec command will 
launch the model using the specified number of processors, utilizing the 
machines in the established ring. The mpdallexit command will terminate the 
ring. If your script is looping over several days, which is typical, you should 
put the mpdboot command before the start of the loop, the mpiexec command inside
the loop, and the mpdallexit command after the point where the loop is 
terminated.

7. Implementing Open-MP (OMP) with an MPI application.

To use a hybrid MPI/OMP approach, follow all of the steps for running an MPI 
application.  In addition, add the environment variable for the number of OMP 
threads to the CAMx job script.

setenv OMP_NUM_THREADS 2

This will utilize both MPI and OMP in the simulation.  The domain will be
divided into slices in the usual way.  But when operating on a slice, the host 
will spawn multiple threads to parallelize the portions of the code where OMP 
parallelization has been included.

